#!/usr/bin/env python3
"""
Automated Job Scraper for Product Management Positions
Modular parser system for different ATS platforms
"""

import requests
from bs4 import BeautifulSoup
import json
import csv
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import os
import logging
import re
from pathlib import Path
from abc import ABC, abstractmethod

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BaseParser(ABC):
    """Abstract base class for job parsers"""
    
    def __init__(self):
        self.product_mgmt_keywords = [
            'product manager', 'product management', 'senior product manager',
            'principal product manager', 'product lead', 'product owner',
            'director of product', 'head of product', 'vp product', 'vp of product',
            'group product manager', 'staff product manager', 'product',
            'pm ', ' pm', 'product strategist', 'product marketing manager'
        ]
    
    @abstractmethod
    def can_parse(self, url: str) -> bool:
        """Check if this parser can handle the given URL"""
        pass
    
    @abstractmethod
    def parse_jobs(self, company: str, url: str) -> List[Dict]:
        """Parse jobs from the given URL"""
        pass
    
    def is_product_management_job(self, title: str, url: str = "", description: str = "") -> bool:
        """Check if job title/description matches Product Management criteria"""
        # Filter out non-job URLs
        excluded_url_patterns = [
            '/product-managers/', '/about-product', '/what-is-product', '/product-management-guide',
            '/blog/', '/resources/', '/tools/', '/templates/', '/product-development/',
            '/product/', '/customers/', '/features/', '/stories/', '/case-studies/',
            '/solutions/', '/pricing/', 'forbes.com', 'techcrunch.com', 'medium.com',
            '/company/', '/about/', '/contact/', '/support/', '.pdf', '.jpg', '.png', '.gif'
        ]
        
        for pattern in excluded_url_patterns:
            if pattern in url.lower():
                return False
        
        # Only consider URLs that look like job listings
        job_url_patterns = [
            '/jobs/', '/careers/', '/job/', '/career/', '/positions/', '/listing/',
            '/opening/', '/vacancy/', '/role/', 'greenhouse.io', 'lever.co', 
            'workday', 'ashbyhq.com'
        ]
        
        if url and not any(pattern in url.lower() for pattern in job_url_patterns):
            return False
        
        text = f"{title.lower()} {description.lower()}"
        has_pm_keyword = any(keyword in text for keyword in self.product_mgmt_keywords)
        
        # Exclude non-PM roles
        exclude_keywords = [
            'software engineer', 'frontend', 'backend', 'developer', 'designer',
            'marketing', 'sales', 'customer success', 'support', 'analyst',
            'intern', 'qa', 'test', 'devops', 'data scientist', 'recruiter',
            'account executive', 'content strategist'
        ]
        
        has_exclude = any(keyword in text for keyword in exclude_keywords)
        return has_pm_keyword and not has_exclude

    def create_job_hash(self, job: Dict) -> str:
        """Create unique hash for job to detect duplicates"""
        unique_string = f"{job['company']}{job['title']}{job['location']}"
        return hashlib.md5(unique_string.encode()).hexdigest()[:12]


class AshbyParser(BaseParser):
    """Parser for Ashby-powered job boards (OpenAI, etc.)"""
    
    def can_parse(self, url: str) -> bool:
        return 'ashbyhq.com' in url
    
    def parse_jobs(self, company: str, url: str) -> List[Dict]:
        """Parse jobs from Ashby GraphQL API"""
        jobs = []
        
        try:
            # Extract organization name from URL
            # https://jobs.ashbyhq.com/openai/?departmentId=... -> openai
            org_match = re.search(r'ashbyhq\.com/([^/?]+)', url)
            if not org_match:
                logger.error(f"Could not extract organization from URL: {url}")
                return jobs
            
            org_name = org_match.group(1)
            
            # Make GraphQL request to Ashby API
            api_url = "https://jobs.ashbyhq.com/api/non-user-graphql?op=ApiJobBoardWithTeams"
            
            query = """
            query ApiJobBoardWithTeams($organizationHostedJobsPageName: String!) {
              jobBoard: jobBoardWithTeams(
                organizationHostedJobsPageName: $organizationHostedJobsPageName
              ) {
                teams {
                  id
                  name
                  parentTeamId
                  __typename
                }
                jobPostings {
                  id
                  title
                  teamId
                  locationId
                  locationName
                  workplaceType
                  employmentType
                  secondaryLocations {
                    locationId
                    locationName
                    __typename
                  }
                  compensationTierSummary
                  __typename
                }
                __typename
              }
            }
            """
            
            payload = {
                "operationName": "ApiJobBoardWithTeams",
                "variables": {"organizationHostedJobsPageName": org_name},
                "query": query
            }
            
            headers = {
                'Content-Type': 'application/json',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'apollographql-client-name': 'frontend_non_user',
                'apollographql-client-version': '0.1.0'
            }
            
            logger.info(f"Making GraphQL request to Ashby for {org_name}")
            response = requests.post(api_url, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if 'data' not in data or not data['data'].get('jobBoard'):
                logger.error(f"No job board data found in response")
                return jobs
            
            job_board = data['data']['jobBoard']
            job_postings = job_board.get('jobPostings', [])
            teams = job_board.get('teams', [])
            
            # Create team lookup
            team_lookup = {team['id']: team['name'] for team in teams}
            
            logger.info(f"Found {len(job_postings)} total jobs, {len(teams)} teams")
            
            # Filter for Product Manager jobs
            for job in job_postings:
                title = job.get('title', '')
                team_id = job.get('teamId', '')
                team_name = team_lookup.get(team_id, '').lower()
                
                # Check if it's a Product Manager role
                if self.is_product_management_job(title, "", "") or 'product management' in team_name:
                    
                    # Build location string
                    location = job.get('locationName', 'Unknown')
                    secondary_locations = job.get('secondaryLocations', [])
                    if secondary_locations:
                        additional_locs = [loc.get('locationName', '') for loc in secondary_locations]
                        location += f" (+ {', '.join(additional_locs)})"
                    
                    # Add workplace type
                    workplace_type = job.get('workplaceType')
                    if workplace_type and workplace_type not in ['null', None]:
                        location += f" - {workplace_type}"
                    
                    # Construct job URL
                    job_url = f"https://jobs.ashbyhq.com/{org_name}/{job.get('id', '')}"
                    
                    job_data = {
                        'company': company,
                        'title': title,
                        'location': location,
                        'url': job_url,
                        'date_posted': 'Unknown',  # Ashby doesn't expose posting dates
                        'date_found': datetime.now().strftime('%Y-%m-%d'),
                        'status': 'active'
                    }
                    job_data['hash'] = self.create_job_hash(job_data)
                    
                    jobs.append(job_data)
                    logger.info(f"‚úÖ Found PM job: {title} ({team_lookup.get(team_id, 'Unknown team')})")
            
            logger.info(f"üìä Total PM jobs found at {company}: {len(jobs)}")
            
        except Exception as e:
            logger.error(f"‚ùå Error parsing Ashby jobs from {company}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
        
        return jobs


class GreenhouseParser(BaseParser):
    """Parser for Greenhouse-powered job boards (Anthropic, etc.)"""
    
    def can_parse(self, url: str) -> bool:
        return 'greenhouse.io' in url
    
    def parse_jobs(self, company: str, url: str) -> List[Dict]:
        """Parse jobs from Greenhouse job boards"""
        jobs = []
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            logger.info(f"Fetching Greenhouse board: {url}")
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Greenhouse-specific selectors
            job_selectors = [
                '.opening', '.job-post', '.position', '[data-job-id]',
                '.job-listing', '.career-listing'
            ]
            
            job_elements = []
            for selector in job_selectors:
                elements = soup.select(selector)
                if elements:
                    job_elements = elements
                    logger.info(f"Found {len(elements)} jobs using selector: {selector}")
                    break
            
            for element in job_elements:
                try:
                    # Extract title
                    title_elem = element.find(['h3', 'h4', 'a']) or element
                    title = title_elem.get_text(strip=True)
                    
                    if not self.is_product_management_job(title):
                        continue
                    
                    # Extract location
                    location_elem = element.find(class_=re.compile(r'location', re.I))
                    location = location_elem.get_text(strip=True) if location_elem else "Unknown"
                    
                    # Extract URL
                    link_elem = element.find('a', href=True)
                    job_url = ""
                    if link_elem:
                        job_url = link_elem.get('href')
                        if not job_url.startswith('http'):
                            from urllib.parse import urljoin
                            job_url = urljoin(url, job_url)
                    
                    job_data = {
                        'company': company,
                        'title': title,
                        'location': location,
                        'url': job_url or url,
                        'date_posted': 'Unknown',
                        'date_found': datetime.now().strftime('%Y-%m-%d'),
                        'status': 'active'
                    }
                    job_data['hash'] = self.create_job_hash(job_data)
                    
                    jobs.append(job_data)
                    logger.info(f"‚úÖ Found Greenhouse PM job: {title}")
                    
                except Exception as e:
                    logger.debug(f"Error parsing Greenhouse job element: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"‚ùå Error parsing Greenhouse jobs from {company}: {e}")
        
        return jobs


class GenericParser(BaseParser):
    """Generic parser for standard websites"""
    
    def can_parse(self, url: str) -> bool:
        # Default parser - can handle any URL
        return True
    
    def parse_jobs(self, company: str, url: str) -> List[Dict]:
        """Parse jobs using BeautifulSoup (your existing logic)"""
        jobs = []
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            logger.info(f"Fetching {url}")
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Your existing generic parsing logic here...
            job_selectors = [
                '.job-listing', '.job-item', '.position', '.job-card', '.job-post',
                '[data-job-id]', '.job', '.career-item', '.opening', '.vacancy',
                '.role', '.position-item', '.job-opportunity'
            ]
            
            job_elements = []
            for selector in job_selectors:
                elements = soup.select(selector)
                if elements:
                    job_elements = elements
                    logger.info(f"Found {len(elements)} job elements using selector: {selector}")
                    break
            
            # Your existing parsing logic...
            # (I'll skip the full implementation for brevity)
            
        except Exception as e:
            logger.error(f"‚ùå Error parsing generic jobs from {company}: {e}")
        
        return jobs


class JobScraper:
    def __init__(self, data_dir: str = "data"):
        """Initialize the job scraper with local file storage"""
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # File paths
        self.jobs_csv = self.data_dir / "jobs.csv"
        self.jobs_json = self.data_dir / "jobs.json"
        self.stats_json = self.data_dir / "stats.json"
        
        # Initialize parsers (order matters - most specific first)
        self.parsers = [
            AshbyParser(),
            GreenhouseParser(),
            GenericParser()  # Always last as fallback
        ]
        
        self.initialize_files()
    
    def get_parser_for_url(self, url: str) -> BaseParser:
        """Get the appropriate parser for a given URL"""
        for parser in self.parsers:
            if parser.can_parse(url):
                logger.info(f"Using {parser.__class__.__name__} for {url}")
                return parser
        
        # Should never reach here due to GenericParser fallback
        return self.parsers[-1]
    
    def scrape_company_jobs(self, company: str, url: str) -> List[Dict]:
        """Scrape jobs from a specific company using appropriate parser"""
        parser = self.get_parser_for_url(url)
        return parser.parse_jobs(company, url)
    
    def initialize_files(self):
        """Create initial data files if they don't exist"""
        if not self.jobs_csv.exists():
            with open(self.jobs_csv, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'company', 'title', 'location', 'url', 'date_posted', 
                    'date_found', 'hash', 'status'
                ])
    
    def load_existing_jobs(self) -> Dict[str, Dict]:
        """Load existing jobs from JSON file"""
        try:
            with open(self.jobs_json, 'r') as f:
                jobs_list = json.load(f)
                return {job['hash']: job for job in jobs_list}
        except Exception as e:
            logger.error(f"Error loading existing jobs: {e}")
            return {}
    
    def save_jobs(self, all_jobs: List[Dict]):
        """Save jobs to both CSV and JSON files"""
        try:
            # Save to JSON (complete data)
            with open(self.jobs_json, 'w') as f:
                json.dump(all_jobs, f, indent=2)
            logger.info(f"‚úÖ Saved {len(all_jobs)} jobs to JSON")
            
            # Save to CSV (for easy viewing)
            with open(self.jobs_csv, 'w', newline='', encoding='utf-8') as f:
                if all_jobs:
                    writer = csv.DictWriter(f, fieldnames=all_jobs[0].keys())
                    writer.writeheader()
                    writer.writerows(all_jobs)
                    logger.info(f"‚úÖ Saved {len(all_jobs)} jobs to CSV")
            
        except Exception as e:
            logger.error(f"‚ùå Error saving jobs: {e}")
    
    def run_scraper(self, companies: Dict[str, str]):
        """Main scraper function"""
        logger.info("üöÄ Starting modular job scraper...")
        
        # Load existing jobs
        existing_jobs = self.load_existing_jobs()
        logger.info(f"Loaded {len(existing_jobs)} existing jobs")
        
        new_jobs = []
        
        for company, url in companies.items():
            logger.info(f"üîç Scraping {company}...")
            try:
                jobs = self.scrape_company_jobs(company, url)
                
                # Filter out existing jobs
                truly_new = [job for job in jobs if job['hash'] not in existing_jobs]
                new_jobs.extend(truly_new)
                
                logger.info(f"‚úÖ {company}: {len(jobs)} PM jobs found, {len(truly_new)} new")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to scrape {company}: {e}")
            
            # Be respectful with delays
            time.sleep(3)
        
        # Combine existing and new jobs
        all_jobs_dict = existing_jobs.copy()
        for job in new_jobs:
            all_jobs_dict[job['hash']] = job
        
        all_jobs = list(all_jobs_dict.values())
        
        # Sort by date found (newest first)
        all_jobs.sort(key=lambda x: x['date_found'], reverse=True)
        
        # Save all jobs
        self.save_jobs(all_jobs)
        
        logger.info(f"üéâ Scraper completed! Found {len(new_jobs)} new jobs out of {len(all_jobs)} total")
        
        return len(new_jobs), len(all_jobs)


def main():
    """Main function"""
    
    # Company URLs with parser auto-detection
    companies = {
        "Stripe": "https://stripe.com/jobs/search?query=product+manager",
        "Notion": "https://www.notion.com/careers?department=product-management#open-positions",
        "Figma": "https://www.figma.com/careers/#job-openings",
        "Linear": "https://linear.app/careers#join-us",
        "Vercel": "https://vercel.com/careers?function=Product",
        "OpenAI": "https://jobs.ashbyhq.com/openai/?departmentId=db3c67d7-3646-4555-925b-40f30ab09f28",
        "Anthropic": "https://job-boards.greenhouse.io/anthropic/?departments%5B%5D=4002057008",
        "Discord": "https://discord.com/careers#all-jobs",
        "Google": "https://www.google.com/about/careers/applications/jobs/results?target_level=DIRECTOR_PLUS&target_level=ADVANCED&q=product%20manager",
    }
    
    try:
        scraper = JobScraper(data_dir="data")
        new_jobs, total_jobs = scraper.run_scraper(companies)
        
        print(f"‚úÖ Scraper completed successfully!")
        print(f"üìä New PM jobs found: {new_jobs}")
        print(f"üìù Total jobs tracked: {total_jobs}")
        print(f"üè¢ Companies scraped: {len(companies)}")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå Scraper failed: {e}")
        logger.exception("Full error details:")
        return 1


if __name__ == "__main__":
    exit(main())
